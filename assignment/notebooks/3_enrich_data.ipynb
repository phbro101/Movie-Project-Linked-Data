{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task: Enrich your dataset with additional data from DBpedia and WikiData \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overview\n",
    "Use the newly found DBpedia and Wikidata resources and collect additional information from DBpedia and Wikidata on the movies. It is sufficient to reuse the DBpedia and Wikidata vocabulary and use directly the DBpedia property values (including when the value is a URL/IRI), e.g., it is enough if you include the following information from DBpedia on the gross value and the producer:\n",
    "\n",
    "<code>    \n",
    "PREFIX dbo: http://dbpedia.org/ontology/\n",
    "    \n",
    "<https://firstname-lastname.org/resource/the_godfather>  dbo:gross    2.541E8^^xsd:double;\n",
    "                                                       dbo:producer <http://dbpedia.org/page/Albert_S._Ruddy> .\n",
    "<code>\n",
    "    \n",
    "> __Hint__: Using SPARQLâ€™s OPTIONAL keyword might help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task Details\n",
    "\n",
    "1. Using SPARQL queries, information you __Must Get__ (if available in DBpedia and Wikidata for the movie):\n",
    "    - Get the distinct genre(s) of a movie\n",
    "    - Get the distinct actors \n",
    "    - Get the homepage of a movie\n",
    "    - Get the number of received awards\n",
    "    - Get the IMDB and the RottenTomatoes links\n",
    "    - Get the box office value/gross value\n",
    "    - Get the cost of a movie\n",
    "\n",
    "\n",
    "\n",
    "## Submission 3:\n",
    "\n",
    "Use RDFLib to load the data you have saved in Task 2 and add the additional information to the corresponding movies. As mentioned above, you can use the DBpedia and WikiData predicates. Save the enriched data set in the output folder with naming __movies_task_3.n3__.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "## Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
